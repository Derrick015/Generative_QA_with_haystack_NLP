{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY2_Nntk_WOQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "n8W5HOZ6YL1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Market Research (Sale of OatmealÂ Cookies)"
      ],
      "metadata": {
        "id": "nvtl3R4_L_1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your gdrive for colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# import the pandas library\n",
        "import pandas as pd \n",
        "# read the Excel file 'Reviews_oatmeal_cookies.xlsx' from Google Drive\n",
        "dat = pd.read_excel('drive/MyDrive/oat_review/Reviews_oatmeal_cookies.xlsx')\n",
        "\n",
        "# display the first three rows of the data\n",
        "dat.head(3)"
      ],
      "metadata": {
        "id": "8bUcpcPUwO0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display information about the dataset\n",
        "dat.info()"
      ],
      "metadata": {
        "id": "pQQ-AWlwM2vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the mean rating\n",
        "dat.Score.mean()"
      ],
      "metadata": {
        "id": "lRaoxk98M5wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the WordCloud and Matplotlib libraries\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Join the 'Summary' column of 'dat' where the 'Score' is less than or equal to 2\n",
        "text = ' '.join(dat[dat.Score <=2].Summary)\n",
        "\n",
        "# Generate a WordCloud object with specified parameters\n",
        "wordcloud = WordCloud(width=800, height=800, background_color='white', min_font_size=10).generate(text)\n",
        "\n",
        "# Create a figure to display the WordCloud object\n",
        "plt.figure(figsize=(8, 8), facecolor=None)\n",
        "\n",
        "# Display the WordCloud object\n",
        "plt.imshow(wordcloud)\n",
        "\n",
        "# Remove the axis ticks and labels\n",
        "plt.axis('off')\n",
        "\n",
        "# Adjust the layout to remove any extra whitespace\n",
        "plt.tight_layout(pad=0)\n",
        "\n",
        "# Display the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "spAkp_9yM5pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join all reviews with line break seperator\n",
        "text1 = '/n'.join(dat.Text)\n",
        "\n",
        "# Open a text file in write mode and save it\n",
        "with open('/content/drive/MyDrive/text_review.txt', 'w') as file:\n",
        "    file.write(text1)\n",
        "\n",
        "# open the file in read mode\n",
        "with open('/content/drive/MyDrive/text_review.txt', 'r') as file:\n",
        "\n",
        "    # read the contents of the file\n",
        "    contents = file.read()\n",
        "\n",
        "    # split the contents into words\n",
        "    words = contents.split()\n",
        "\n",
        "    # count the number of words\n",
        "    num_words = len(words)\n",
        "\n",
        "# print the number of words\n",
        "print(\"The file contains\", num_words, \"words.\")"
      ],
      "metadata": {
        "id": "lF5-pJwPM5jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upgrade pip\n",
        "# ! pip install --upgrade pip\n",
        "\n",
        "# install farm-haystack along with with its dependencies for google colab environment\n",
        "# ! pip install farm-haystack[colab]\n",
        "\n",
        "# Import the TextConverter to convert text to documents\n",
        "from haystack.nodes import TextConverter\n",
        "\n",
        "# Set the directory path for the input text file\n",
        "DOC_DIR = '/content/drive/MyDrive/text_review.txt'\n",
        "\n",
        "# Initialize a TextConverter object with the specified parameters\n",
        "converter = TextConverter(\n",
        "    remove_numeric_tables=True,  # Remove numeric tables from the text\n",
        "    valid_languages=[\"en\"]  # Specify that the text is in English\n",
        ")\n",
        "\n",
        "# Convert the text file to a list of Haystack Document objects and extract metadata\n",
        "# The convert() method returns a tuple with the list of documents and the metadata dictionary\n",
        "docs = converter.convert(file_path=DOC_DIR, meta=None)[0]"
      ],
      "metadata": {
        "id": "uaXQwEszM5Ys"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing PreProcessor class from the haystack.nodes module\n",
        "from haystack.nodes import PreProcessor\n",
        "\n",
        "# Creating an instance of the PreProcessor class with various options\n",
        "preprocessor = PreProcessor(\n",
        "    clean_empty_lines=True,\n",
        "    clean_whitespace=True,\n",
        "    clean_header_footer=True,\n",
        "    split_by=\"word\",\n",
        "    split_length=100,\n",
        "    split_overlap=3,\n",
        "    split_respect_sentence_boundary=False,\n",
        ")\n",
        "\n",
        "# Using the process method of the preprocessor instance to process the docs\n",
        "processed_docs = preprocessor.process(docs)"
      ],
      "metadata": {
        "id": "fUVPl6hNM4fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install farm-haystack along with with its dependencies for faiss\n",
        "# NB: You may need to restart the run time after installation before importing FAISSDocumentStore\n",
        "\n",
        "# !pip install farm-haystack[faiss]\n",
        "\n",
        "# Importing the FAISSDocumentStore class from the haystack.document_stores module\n",
        "from haystack.document_stores import FAISSDocumentStore\n",
        "\n",
        "# Creating an instance of the FAISSDocumentStore class with specified options\n",
        "document_store = FAISSDocumentStore(\n",
        "    faiss_index_factory_str=\"Flat\",\n",
        "    embedding_dim=1536\n",
        ")"
      ],
      "metadata": {
        "id": "nuiOF8u8NgMn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete documents in database\n",
        "document_store.delete_documents()\n",
        "\n",
        "# add preprocessed document\n",
        "document_store.write_documents(processed_docs)"
      ],
      "metadata": {
        "id": "AChNsEodNgHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the EmbeddingRetriever class from the haystack.nodes module\n",
        "# To get OpenAI API key: https://openai.com/product\n",
        "\n",
        "from haystack.nodes import EmbeddingRetriever\n",
        "\n",
        "# Set the OpenAI API key\n",
        "MY_API_KEY = \"enter api key from open ai\"\n",
        "\n",
        "# Initialize an EmbeddingRetriever object with the specified parameters\n",
        "retriever = EmbeddingRetriever(\n",
        "    document_store=document_store,  # Document database\n",
        "    embedding_model=\"text-embedding-ada-002\",  # Pre-trained text embedding model\n",
        "    batch_size=32,  # Batch size for processing data\n",
        "    api_key=MY_API_KEY,  # OpenAI API key for authentication\n",
        "    max_seq_len=1024  # Maximum length of input sequences\n",
        ")\n",
        "\n",
        "# Update the embeddings of documents in the document store using the retriever object\n",
        "document_store.update_embeddings(retriever)"
      ],
      "metadata": {
        "id": "wmdRrWo-NgAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the OpenAIAnswerGenerator class from the haystack.nodes module\n",
        "from haystack.nodes import OpenAIAnswerGenerator\n",
        "\n",
        "# Initialize an OpenAIAnswerGenerator object with the specified parameters\n",
        "generator = OpenAIAnswerGenerator(\n",
        "    api_key=MY_API_KEY,  # OpenAI API key for authentication\n",
        "    model=\"text-davinci-003\",  # Open AI text model\n",
        "    temperature=.5,  # Controls the randomness of the generated responses\n",
        "    max_tokens=100  # Maximum number of tokens (words) in the generated responses\n",
        ")"
      ],
      "metadata": {
        "id": "XXhPyr1CNf5f"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the GenerativeQAPipeline class from the haystack.pipelines module\n",
        "from haystack.pipelines import GenerativeQAPipeline\n",
        "\n",
        "# Initialize a GenerativeQAPipeline object with the specified parameters\n",
        "gpt_search_engine = GenerativeQAPipeline(\n",
        "    generator=generator,  # Answer generator object\n",
        "    retriever=retriever  # Retriever object\n",
        ")"
      ],
      "metadata": {
        "id": "cdQ-8tLPOjA_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query 1: What are some complaints about theÂ cookies?"
      ],
      "metadata": {
        "id": "0raPAJ59OrKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the print_answers function from the haystack.utils module\n",
        "from haystack.utils import print_answers\n",
        "\n",
        "# Set the input query string\n",
        "query_input = \"What are some complaints about the cookies?\"\n",
        "query = query_input\n",
        "\n",
        "# Define the search parameters as a dictionary with retriever and generator parameters\n",
        "params = {\n",
        "    \"Retriever\": {\"top_k\": 15},  # Retrieve the top 15 most relevant documents\n",
        "    \"Generator\": {\"top_k\": 1}  # Generate the top 1 most likely answer\n",
        "}\n",
        "\n",
        "# Use the GenerativeQAPipeline object to answer the input query with the specified search parameters\n",
        "answer = gpt_search_engine.run(query=query, params=params)\n",
        "\n",
        "# Print the predicted answers with minimal details\n",
        "print_answers(answer, details=\"minimum\")"
      ],
      "metadata": {
        "id": "J8vThyKGHd9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query 2: What else are people saying about the cookie'sÂ dryness?"
      ],
      "metadata": {
        "id": "e7umetjLPJmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the print_answers function from the haystack.utils module\n",
        "from haystack.utils import print_answers\n",
        "\n",
        "# Set the input query string\n",
        "query_input = \"What else are people saying about the cookie's dryness?\"\n",
        "query = query_input\n",
        "\n",
        "# Define the search parameters as a dictionary with retriever and generator parameters\n",
        "params = {\n",
        "    \"Retriever\": {\"top_k\": 15},  # Retrieve the top 15 most relevant documents\n",
        "    \"Generator\": {\"top_k\": 1}  # Generate the top 1 most likely answer\n",
        "}\n",
        "\n",
        "# Use the GenerativeQAPipeline object to answer the input query with the specified search parameters\n",
        "answer = gpt_search_engine.run(query=query, params=params)\n",
        "\n",
        "# Print the predicted answers with minimal details\n",
        "print_answers(answer, details=\"minimum\")"
      ],
      "metadata": {
        "id": "evJ4MFYaPJfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query 3: What are some customer recommendations to improve theÂ product?"
      ],
      "metadata": {
        "id": "in8JgxUAPJYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the print_answers function from the haystack.utils module\n",
        "from haystack.utils import print_answers\n",
        "\n",
        "# Set the input query string\n",
        "query_input = \"What are some customer recommendations to improve the product?\"\n",
        "query = query_input\n",
        "\n",
        "# Define the search parameters as a dictionary with retriever and generator parameters\n",
        "params = {\n",
        "    \"Retriever\": {\"top_k\": 15},  # Retrieve the top 15 most relevant documents\n",
        "    \"Generator\": {\"top_k\": 1}  # Generate the top 1 most likely answer\n",
        "}\n",
        "\n",
        "# Use the GenerativeQAPipeline object to answer the input query with the specified search parameters\n",
        "answer = gpt_search_engine.run(query=query, params=params)\n",
        "\n",
        "# Print the predicted answers with minimal details\n",
        "print_answers(answer, details=\"minimum\")"
      ],
      "metadata": {
        "id": "hXVCJP1DQhHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Inquiry System (UK Net Zero Policy Documents)"
      ],
      "metadata": {
        "id": "JSkC9_o6Qg-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the 'farm-haystack' package with PDF support\n",
        "# ! pip install 'farm-haystack[pdf]'\n",
        "\n",
        "# Import the PDFToTextConverter class from haystack.nodes module\n",
        "from haystack.nodes import PDFToTextConverter\n",
        "\n",
        "# Import the convert_files_to_docs function from haystack.utils module\n",
        "from haystack.utils import convert_files_to_docs\n",
        "\n",
        "# Set the directory path where the documents to be converted are located\n",
        "DOC_DIR = '/content/drive/MyDrive/net_zero'\n",
        "\n",
        "# Use the convert_files_to_docs function to convert the PDF documents in the directory to text\n",
        "# The split_paragraphs parameter specifies whether to split the text into paragraphs or not\n",
        "docs = convert_files_to_docs(dir_path=DOC_DIR, split_paragraphs=True)"
      ],
      "metadata": {
        "id": "e4Y1a15xQg3K"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing PreProcessor class from the haystack.nodes module\n",
        "from haystack.nodes import PreProcessor\n",
        "\n",
        "# Creating an instance of the PreProcessor class with various options\n",
        "preprocessor = PreProcessor(\n",
        "    clean_empty_lines=True,\n",
        "    clean_whitespace=True,\n",
        "    clean_header_footer=True,\n",
        "    split_by=\"word\",\n",
        "    split_length=100,\n",
        "    split_overlap=3,\n",
        "    split_respect_sentence_boundary=False,\n",
        ")\n",
        "\n",
        "# Using the process method of the preprocessor instance to process the docs\n",
        "processed_docs = preprocessor.process(docs)\n",
        "\n",
        "# install farm-haystack along with with its dependencies for faiss\n",
        "# !pip install farm-haystack[faiss]\n",
        "\n",
        "# Importing the FAISSDocumentStore class from the haystack.document_stores module\n",
        "from haystack.document_stores import FAISSDocumentStore\n",
        "\n",
        "# Creating an instance of the FAISSDocumentStore class with specified options\n",
        "# NB: Remove the previous faiss_document_store.db from the previous project or run this section in a new directory\n",
        "\n",
        "document_store = FAISSDocumentStore(\n",
        "    faiss_index_factory_str=\"Flat\",\n",
        "    embedding_dim=1536\n",
        ")\n",
        "\n",
        "# delete documents in database\n",
        "document_store.delete_documents()\n",
        "\n",
        "# add preprocessed document\n",
        "document_store.write_documents(processed_docs)\n",
        "\n",
        "# Import the EmbeddingRetriever class from the haystack.nodes module\n",
        "from haystack.nodes import EmbeddingRetriever\n",
        "\n",
        "# Set the OpenAI API key\n",
        "MY_API_KEY = \"enter api key from open ai\"\n",
        "\n",
        "# Initialize an EmbeddingRetriever object with the specified parameters\n",
        "retriever = EmbeddingRetriever(\n",
        "    document_store=document_store,  # Document database\n",
        "    embedding_model=\"text-embedding-ada-002\",  # Pre-trained text embedding model\n",
        "    batch_size=32,  # Batch size for processing data\n",
        "    api_key=MY_API_KEY,  # OpenAI API key for authentication\n",
        "    max_seq_len=1024  # Maximum length of input sequences\n",
        ")\n",
        "\n",
        "# Update the embeddings of documents in the document store using the retriever object\n",
        "document_store.update_embeddings(retriever)\n",
        "\n",
        "# Import the OpenAIAnswerGenerator class from the haystack.nodes module\n",
        "from haystack.nodes import OpenAIAnswerGenerator\n",
        "\n",
        "# Initialize an OpenAIAnswerGenerator object with the specified parameters\n",
        "generator = OpenAIAnswerGenerator(\n",
        "    api_key=MY_API_KEY,  # OpenAI API key for authentication\n",
        "    model=\"text-davinci-003\",  # Open AI text model\n",
        "    temperature=.5,  # Controls the randomness of the generated responses\n",
        "    max_tokens=100  # Maximum number of tokens (words) in the generated responses\n",
        ")\n",
        "\n",
        "# Import the GenerativeQAPipeline class from the haystack.pipelines module\n",
        "from haystack.pipelines import GenerativeQAPipeline\n",
        "\n",
        "# Initialize a GenerativeQAPipeline object with the specified parameters\n",
        "gpt_search_engine = GenerativeQAPipeline(\n",
        "    generator=generator,  # Answer generator object\n",
        "    retriever=retriever  # Retriever object\n",
        ")"
      ],
      "metadata": {
        "id": "5G4S7RtVQggd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query 1: What does net zero evenÂ mean?"
      ],
      "metadata": {
        "id": "RNkpIeQaSH3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the print_answers function from the haystack.utils module\n",
        "from haystack.utils import print_answers\n",
        "\n",
        "# Set the input query string\n",
        "query_input = \"What does net zero even mean?\"\n",
        "query = query_input\n",
        "\n",
        "# Define the search parameters as a dictionary with retriever and generator parameters\n",
        "params = {\n",
        "    \"Retriever\": {\"top_k\": 15},  # Retrieve the top 15 most relevant documents\n",
        "    \"Generator\": {\"top_k\": 1}  # Generate the top 1 most likely answer\n",
        "}\n",
        "\n",
        "# Use the GenerativeQAPipeline object to answer the input query with the specified search parameters\n",
        "answer = gpt_search_engine.run(query=query, params=params)\n",
        "\n",
        "# Print the predicted answers with minimal details\n",
        "print_answers(answer, details=\"minimum\")"
      ],
      "metadata": {
        "id": "2Wqx5lIjSDrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query 2: When does the UK intend to reach netÂ zero? "
      ],
      "metadata": {
        "id": "aPBIvCVhSOGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the print_answers function from the haystack.utils module\n",
        "from haystack.utils import print_answers\n",
        "\n",
        "# Set the input query string\n",
        "query_input = \"When does the UK intend to reach net zero?\"\n",
        "query = query_input\n",
        "\n",
        "# Define the search parameters as a dictionary with retriever and generator parameters\n",
        "params = {\n",
        "    \"Retriever\": {\"top_k\": 15},  # Retrieve the top 15 most relevant documents\n",
        "    \"Generator\": {\"top_k\": 1}  # Generate the top 1 most likely answer\n",
        "}\n",
        "\n",
        "# Use the GenerativeQAPipeline object to answer the input query with the specified search parameters\n",
        "answer = gpt_search_engine.run(query=query, params=params)\n",
        "\n",
        "# Print the predicted answers with minimal details\n",
        "print_answers(answer, details=\"minimum\")"
      ],
      "metadata": {
        "id": "xmR-Rp-fSOZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query 3: I am saving to buy a new petrol car in 2038, is there any information about how feasible this may be in thatÂ year?"
      ],
      "metadata": {
        "id": "L6D3z_3tShvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the print_answers function from the haystack.utils module\n",
        "from haystack.utils import print_answers\n",
        "\n",
        "# Set the input query string\n",
        "query_input = \"I am saving to buy a new petrol car in 2038, is there any information about how feasible this may be in that year?\"\n",
        "query = query_input\n",
        "\n",
        "# Define the search parameters as a dictionary with retriever and generator parameters\n",
        "params = {\n",
        "    \"Retriever\": {\"top_k\": 15},  # Retrieve the top 15 most relevant documents\n",
        "    \"Generator\": {\"top_k\": 1}  # Generate the top 1 most likely answer\n",
        "}\n",
        "\n",
        "# Use the GenerativeQAPipeline object to answer the input query with the specified search parameters\n",
        "answer = gpt_search_engine.run(query=query, params=params)\n",
        "\n",
        "# Print the predicted answers with minimal details\n",
        "print_answers(answer, details=\"minimum\")"
      ],
      "metadata": {
        "id": "o_6GxjqhSiIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query 4: The government will take away my diesel car in 2030. This has been my only car for 10 years and I will hate it if the government takes itÂ away"
      ],
      "metadata": {
        "id": "_Bf36_rrS2vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the print_answers function from the haystack.utils module\n",
        "from haystack.utils import print_answers\n",
        "\n",
        "# Set the input query string\n",
        "query_input = \"The government will take away my diesel car in 2030. This has been my only car for 10 years and I will hate it if the government takes it away\"\n",
        "query = query_input\n",
        "\n",
        "# Define the search parameters as a dictionary with retriever and generator parameters\n",
        "params = {\n",
        "    \"Retriever\": {\"top_k\": 15},  # Retrieve the top 15 most relevant documents\n",
        "    \"Generator\": {\"top_k\": 1}  # Generate the top 1 most likely answer\n",
        "}\n",
        "\n",
        "# Use the GenerativeQAPipeline object to answer the input query with the specified search parameters\n",
        "answer = gpt_search_engine.run(query=query, params=params)\n",
        "\n",
        "# Print the predicted answers with minimal details\n",
        "print_answers(answer, details=\"minimum\")"
      ],
      "metadata": {
        "id": "FqovwEP1S6p_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query 5: What information is there about the large fans that would cool the atmosphere and stop climateÂ change?"
      ],
      "metadata": {
        "id": "bfrDblvrWWpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the print_answers function from the haystack.utils module\n",
        "from haystack.utils import print_answers\n",
        "\n",
        "# Set the input query string\n",
        "query_input = \"What information is there about the large fans that would cool the atmosphere and stop climate change?\"\n",
        "query = query_input\n",
        "\n",
        "# Define the search parameters as a dictionary with retriever and generator parameters\n",
        "params = {\n",
        "    \"Retriever\": {\"top_k\": 15},  # Retrieve the top 15 most relevant documents\n",
        "    \"Generator\": {\"top_k\": 1}  # Generate the top 1 most likely answer\n",
        "}\n",
        "\n",
        "# Use the GenerativeQAPipeline object to answer the input query with the specified search parameters\n",
        "answer = gpt_search_engine.run(query=query, params=params)\n",
        "\n",
        "# Print the predicted answers with minimal details\n",
        "print_answers(answer, details=\"minimum\")"
      ],
      "metadata": {
        "id": "47H8zUUMS7KO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}